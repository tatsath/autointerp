================================================================================
ANALYSIS: How Feature Explanations Are Generated
Why Labels May Be Too Generic
================================================================================

This document analyzes the actual code and data flow to understand what is sent
to the explainer LLM and why explanations might be too generic.

================================================================================
FINDING 1: WHAT IS ACTUALLY SENT TO THE LLM
================================================================================

The explainer uses `NPMaxActExplainer` which sends examples in this format:

1. For each of the top 24 examples (k_max_act=24):
   - Finds the token with MAXIMUM activation in that example
   - Extracts:
     * `left_context`: 12 tokens BEFORE the max activation token
     * `token`: The single token at max activation
     * `right_context`: 12 tokens AFTER the max activation token
   - Joins tokens WITHOUT SPACES: `"".join(str_tokens[...])`
   
2. Sends as JSON:
```json
{
  "feature_id": "latent_1532",
  "max_act_examples": [
    {
      "token": "dividend",
      "left_context": "MunicipalIncomeFunddeclares$0.01special",
      "right_context": "CardiometodivestCanadianbusinesstoC",
      "activation": 9.625
    },
    ...
  ]
}
```

3. System prompt: `SYSTEM_CONCISE` (from np_max_act_explainer.py)
   - Asks for ≤18 words
   - Emphasizes specificity but may not be strict enough

================================================================================
FINDING 2: KEY ISSUES IDENTIFIED
================================================================================

ISSUE 1: TOKENS JOINED WITHOUT SPACES
--------------------------------------
Code: `left_context = "".join(str_tokens[start_idx:max_act_idx])`
Problem: Tokens are concatenated directly, making them hard to parse
Example: "MunicipalIncomeFunddeclares" instead of "Municipal Income Fund declares"
Impact: LLM may struggle to identify word boundaries and semantic patterns

ISSUE 2: ONLY ONE TOKEN HIGHLIGHTED PER EXAMPLE
------------------------------------------------
Code: Only the token at `max_act_idx` is shown as the "token" field
Problem: Other activating tokens in the context are NOT marked
Impact: LLM only sees ONE highlighted token per example, missing patterns where
        multiple tokens activate together

ISSUE 3: CONTEXT IS SPLIT (LEFT + RIGHT)
-----------------------------------------
Code: `left_context` (12 tokens) + `token` + `right_context` (12 tokens)
Problem: Semantic coherence may be lost when context is split
Impact: LLM may not see complete phrases or sentences, only fragments

ISSUE 4: WINDOW SIZE MAY BE TOO SMALL
--------------------------------------
Code: `self.window = 12` (default)
Problem: Only ±12 tokens of context around max activation
Impact: May miss broader semantic patterns that span more tokens

ISSUE 5: PROMPT MAY NOT EMPHASIZE SPECIFICITY STRONGLY ENOUGH
--------------------------------------------------------------
Current prompt says:
- "Be SPECIFIC and CONCISE (≤ 18 words)"
- "AVOID generic terms"
- "Focus on SPECIFIC CONCEPTS with CONTEXT"

But it doesn't:
- Explicitly forbid generic phrases like "declarations by funds"
- Require domain-specific terminology
- Penalize explanations that could apply to many features

================================================================================
FINDING 3: DISCREPANCY WITH END-TO-END EXPLANATION FILES
================================================================================

The end-to-end explanation files (feature_1532_end_to_end_explanation.txt) show:
- 32-token windows with ALL activating tokens marked with <<token>>
- Full context windows

BUT THIS IS NOT WHAT'S ACTUALLY SENT TO THE LLM!

The end-to-end explanation file is generated by `explain_explainer_input.py` which:
- Loads the examples from the dataset
- Formats them for VISUALIZATION (showing all tokens with <<markers>>)
- This is just for human inspection, NOT what the explainer sees

The actual explainer receives:
- JSON with left_context + token + right_context
- Only ONE token highlighted per example
- 12 tokens of context on each side (not 32)

================================================================================
FINDING 4: WHY EXPLANATIONS ARE GENERIC
================================================================================

Based on the code analysis, explanations may be generic because:

1. LIMITED CONTEXT VISIBILITY
   - LLM only sees ±12 tokens around ONE highlighted token
   - Cannot see full semantic patterns that span longer sequences
   - May default to generic descriptions when context is unclear

2. TOKEN CONCATENATION ISSUES
   - Tokens joined without spaces make word boundaries unclear
   - Harder for LLM to identify specific phrases or concepts
   - May lead to generic interpretations

3. SINGLE TOKEN FOCUS
   - Only one token highlighted per example
   - Cannot see patterns where multiple tokens activate together
   - May miss multi-token concepts (e.g., "special dividend" as a phrase)

4. PROMPT LIMITATIONS
   - Prompt asks for ≤18 words but doesn't strongly penalize generic terms
   - Examples in prompt may not be specific enough
   - No explicit requirement for domain-specific terminology

5. PATTERN DETECTION DIFFICULTY
   - With fragmented context (left + right split), LLM may struggle to
     identify complete semantic patterns
   - May default to generic descriptions when patterns are unclear

================================================================================
FINDING 5: COMPARISON WITH EXPECTED FORMAT
================================================================================

What the user expects (based on end-to-end explanation files):
- Full 32-token windows
- ALL activating tokens marked
- Complete semantic context

What the explainer actually receives:
- Split context: 12 tokens left + 1 token + 12 tokens right
- Only ONE token highlighted per example
- Tokens joined without spaces

This mismatch explains why:
- Explanations seem generic (limited context)
- Patterns may be missed (only one token highlighted)
- Semantic coherence is lost (split context)

================================================================================
RECOMMENDATIONS (FOR FUTURE IMPROVEMENTS)
================================================================================

1. FIX TOKEN FORMATTING
   - Add spaces between tokens: `" ".join(str_tokens[...])`
   - Or use tokenizer's proper decoding

2. HIGHLIGHT ALL ACTIVATING TOKENS
   - Mark all tokens above threshold, not just max activation
   - Use <<token>> markers like in visualization

3. INCREASE CONTEXT WINDOW
   - Use full 32-token windows instead of ±12 tokens
   - Or make window size configurable

4. IMPROVE PROMPT SPECIFICITY
   - Add explicit examples of BAD generic explanations
   - Require domain-specific terminology
   - Penalize explanations that could apply to multiple features

5. SEND COMPLETE CONTEXT
   - Send full token sequences, not split left/right
   - Mark all activating tokens with <<markers>>
   - Preserve semantic coherence

6. VALIDATE EXPLANATION SPECIFICITY
   - Add post-processing to detect generic explanations
   - Require minimum specificity score
   - Re-generate if explanation is too generic

================================================================================
CONCLUSION
================================================================================

The labels are generic because:

1. The explainer receives LIMITED CONTEXT (±12 tokens, split left/right)
2. Only ONE token is highlighted per example (missing multi-token patterns)
3. Tokens are joined WITHOUT SPACES (harder to parse)
4. The prompt may not emphasize specificity strongly enough
5. The format sent to LLM differs from what's shown in visualization files

The end-to-end explanation files show a DIFFERENT format (32-token windows with
all tokens marked) which is only for visualization, not what's actually sent
to the explainer LLM.

To fix this, the code should:
- Send full context windows (not split)
- Mark all activating tokens (not just max)
- Add proper spacing between tokens
- Strengthen prompt to require domain-specific terminology

================================================================================

