================================================================================
FEATURE EXPLANATION vs ACTIVATING WORDS ANALYSIS
Understanding the Discrepancy Between LLM Explanations and Token Frequencies
================================================================================

EXECUTIVE SUMMARY
--------------------------------------------------------------------------------

This document explains why feature explanations (generated by LLM explainers) may differ
from the top activating words (from statistical token analysis). The key insight is:

1. EXPLANATIONS = Semantic interpretation of full context
2. TOP WORDS = Statistical frequency of individual tokens

They complement each other but measure different things.

================================================================================
HOW EXPLANATIONS ARE GENERATED
================================================================================


The LLM explainer (Qwen/Qwen2.5-72B-Instruct) receives:

1. TOP 15-25 STRONGEST ACTIVATING EXAMPLES
   - Full text sequences (32 tokens of context)
   - Examples centered on the activation point
   - Each example shows which tokens activated (marked with <<token>>)

2. THE PROMPT ASKS:
   "Look at the parts of the document the neuron activates for and summarize
   in a single sentence what the neuron is activating on. Try not to be overly
   specific. Your explanation should cover most or all activating words."

3. THE LLM DOES:
   - Semantic abstraction across examples
   - Pattern recognition in context
   - Generalization to a concept
   - NOT just word counting


================================================================================
HOW TOP WORDS ARE CALCULATED
================================================================================


The token analysis:

1. SAMPLES TOP 50,000 STRONGEST ACTIVATIONS
   - Individual token positions
   - No surrounding context
   - Just the token ID and activation value

2. COUNTS FREQUENCY
   - How many times each token appears
   - Maximum activation per token
   - Average activation per token

3. RANKS BY ACTIVATION STRENGTH
   - Sorted by max_activation, then count
   - Shows which tokens fire most strongly
   - BUT loses semantic context


================================================================================
FEATURE-BY-FEATURE ANALYSIS
================================================================================


================================================================================
Feature 18529
================================================================================

Explanation: "Smart Beta Equity ETPs globally increased"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------



================================================================================
Feature 6105
================================================================================

Explanation: "Earnings beats relative to estimates"

Top 15 Activating Words:
--------------------------------------------------------------------------------
 1. 'Newsp'                         max= 25.12  count=    3
 2. 'Isn'                           max= 25.00  count=    4
 3. 'Vers'                          max= 24.88  count=    3
 4. 'Is'                            max= 24.75  count=   86
 5. 'Among'                         max= 24.75  count=   41
 6. 'Restaur'                       max= 24.75  count=    5
 7. 'Year'                          max= 24.62  count=   55
 8. 'Manufacturing'                 max= 24.62  count=   31
 9. 'Should'                        max= 24.62  count=   29
10. 'says'                          max= 24.62  count=   29
11. 'Season'                        max= 24.62  count=   16
12. 'Education'                     max= 24.62  count=    7
13. 'German'                        max= 24.62  count=    4
14. 'Us'                            max= 24.62  count=    2
15. 'bors'                          max= 24.62  count=    2

Analysis:
--------------------------------------------------------------------------------
The explanation '"Earnings beats relative to estimates"' captures a semantic pattern that may not
be obvious from individual words. The top words show frequent tokens,
but the explanation shows what those tokens MEAN IN CONTEXT.

For example, words like 'Newsp' may appear
frequently, but when seen in full context across multiple examples, the
LLM recognizes a pattern related to '"Earnings beats relative to estimates"'.



================================================================================
Feature 8982
================================================================================

Explanation: "M&A topping bid announcements"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------



================================================================================
Feature 2216
================================================================================

Explanation: "Earnings misses relative to estimates"

Top 15 Activating Words:
--------------------------------------------------------------------------------
 1. 'York'                          max= 67.50  count=   25
 2. 'November'                      max= 67.00  count=   13
 3. 'rage'                          max= 67.00  count=    4
 4. 'bles'                          max= 67.00  count=    2
 5. 'III'                           max= 67.00  count=    2
 6. 'Abb'                           max= 67.00  count=    1
 7. 'iss'                           max= 67.00  count=    1
 8. 'ons'                           max= 66.50  count=   13
 9. 'nd'                            max= 66.50  count=   13
10. 'December'                      max= 66.50  count=   11
11. 'September'                     max= 66.50  count=    9
12. 'January'                       max= 66.50  count=    7
13. 'July'                          max= 66.50  count=    7
14. 'Third'                         max= 66.50  count=    5
15. 'March'                         max= 66.50  count=    4

Analysis:
--------------------------------------------------------------------------------
The explanation '"Earnings misses relative to estimates"' captures a semantic pattern that may not
be obvious from individual words. The top words show frequent tokens,
but the explanation shows what those tokens MEAN IN CONTEXT.

For example, words like 'York' may appear
frequently, but when seen in full context across multiple examples, the
LLM recognizes a pattern related to '"Earnings misses relative to estimates"'.



================================================================================
Feature 19903
================================================================================

Explanation: "Analyst coverage and earnings calls"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------



================================================================================
Feature 18072
================================================================================

Explanation: "Earnings misses relative to estimates"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------



================================================================================
Feature 23314
================================================================================

Explanation: "Earnings misses relative to estimates"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------



================================================================================
Feature 29064
================================================================================

Explanation: "Punctuation in financial data separation"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------



================================================================================
Feature 33546
================================================================================

Explanation: "Dividend declaration amounts specified"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------



================================================================================
Feature 13092
================================================================================

Explanation: "Quarterly loss reporting and revenue"

Top 15 Activating Words:
--------------------------------------------------------------------------------

Analysis:
--------------------------------------------------------------------------------


================================================================================
CONCLUSION
================================================================================


KEY TAKEAWAYS:

1. EXPLANATIONS ARE SEMANTIC, NOT STATISTICAL
   - They interpret meaning across examples
   - They capture patterns in context
   - They generalize to concepts

2. TOP WORDS ARE STATISTICAL, NOT SEMANTIC
   - They show frequency and strength
   - They lose context
   - They show WHAT fires, not WHAT IT MEANS

3. BOTH ARE VALID AND COMPLEMENTARY
   - Top words: "What tokens activate?"
   - Explanation: "What semantic pattern do they represent?"

4. WHY THEY DIFFER
   - Common words (like "Industry", "Third", "Should") may appear frequently
   - But in context, they form patterns (like "Special dividend declarations")
   - The LLM explainer sees the forest, token analysis sees the trees

5. WHEN TO TRUST WHICH
   - Trust explanations for: Understanding what the feature captures semantically
   - Trust top words for: Understanding which specific tokens trigger it
   - Use both for: Complete picture of feature behavior


================================================================================
METHODOLOGY
================================================================================


This analysis was generated by:
- Loading cached activations from safetensors files
- Sampling top 50,000 activations per feature
- Extracting top 15-30 meaningful words (filtered for letters)
- Comparing with LLM-generated explanations from autointerp_full

The explanation model saw:
- Top 15-25 strongest activating examples
- Full 32-token context windows
- Examples centered on activation points
- Semantic similarity-based negative examples


