"""
ExLlamaV2 Client - Much faster than VLLM
3-5x faster inference with lower memory usage
"""

import asyncio
import torch
from dataclasses import dataclass
from typing import Union, List
import gc

from .client import Client, Response

@dataclass
class ExLlamaV2Response(Response):
    pass

class ExLlamaV2Client(Client):
    provider = "exllamav2"

    def __init__(self, model: str, max_memory: float = 0.7, max_model_len: int = 2048, num_gpus: int = 1):
        super().__init__(model)
        self.max_model_len = max_model_len
        self.num_gpus = num_gpus
        self.max_memory = max_memory
        
        # Import ExLlamaV2 components
        try:
            from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache, ExLlamaV2Tokenizer
            from exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2BaseGenerator
            self.ExLlamaV2 = ExLlamaV2
            self.ExLlamaV2Config = ExLlamaV2Config
            self.ExLlamaV2Cache = ExLlamaV2Cache
            self.ExLlamaV2Tokenizer = ExLlamaV2Tokenizer
            self.ExLlamaV2Sampler = ExLlamaV2Sampler
            self.ExLlamaV2BaseGenerator = ExLlamaV2BaseGenerator
        except ImportError:
            raise ImportError("ExLlamaV2 not installed. Install with: pip install exllamav2")
        
        # Initialize model
        self._initialize_model()

    def _initialize_model(self):
        """Initialize ExLlamaV2 model with optimized settings"""
        print(f"üöÄ Loading {self.model} with ExLlamaV2 (much faster than VLLM)...")
        
        # Configure model
        config = self.ExLlamaV2Config()
        config.model_dir = self.model
        config.prepare()
        
        # Set memory limits
        config.max_seq_len = self.max_model_len
        config.max_batch_size = 1  # Single batch for speed
        
        # Initialize model
        self.model_instance = self.ExLlamaV2(config)
        self.model_instance.load()
        
        # Initialize tokenizer
        self.tokenizer = self.ExLlamaV2Tokenizer(config)
        
        # Initialize cache
        self.cache = self.ExLlamaV2Cache(self.model_instance)
        
        # Initialize generator
        self.generator = self.ExLlamaV2BaseGenerator(self.model_instance, self.cache, self.tokenizer)
        
        # Configure sampler for speed
        self.sampler = self.ExLlamaV2Sampler()
        self.sampler.temperature = 0.7
        self.sampler.top_p = 0.9
        self.sampler.top_k = 50
        self.sampler.token_repetition_penalty = 1.1
        
        print("‚úÖ ExLlamaV2 model loaded successfully!")

    async def generate(self, prompt: Union[str, List[dict]], **kwargs) -> str | Response:
        """Generate text using ExLlamaV2 (much faster than VLLM)"""
        
        # Convert chat format if needed
        if isinstance(prompt, list):
            # Convert chat messages to string
            prompt_text = ""
            for message in prompt:
                if message.get("role") == "user":
                    prompt_text += f"User: {message['content']}\n"
                elif message.get("role") == "assistant":
                    prompt_text += f"Assistant: {message['content']}\n"
            prompt_text += "Assistant:"
        else:
            prompt_text = prompt
        
        # Tokenize input
        input_ids = self.tokenizer.encode(prompt_text)
        
        # Truncate if too long
        if len(input_ids) > self.max_model_len - 100:  # Leave room for generation
            input_ids = input_ids[:self.max_model_len - 100]
        
        # Generate
        try:
            # Clear cache for fresh generation
            self.cache.current_seq_len = 0
            
            # Generate tokens
            max_new_tokens = kwargs.get("max_new_tokens", 100)
            temperature = kwargs.get("temperature", 0.7)
            top_p = kwargs.get("top_p", 0.9)
            
            # Update sampler settings
            self.sampler.temperature = temperature
            self.sampler.top_p = top_p
            
            # Generate
            generated_ids = self.generator.generate_simple(
                input_ids,
                self.sampler,
                max_new_tokens,
                stop_token_ids=[self.tokenizer.eos_token_id]
            )
            
            # Decode response
            response_text = self.tokenizer.decode(generated_ids[len(input_ids):])
            
            return ExLlamaV2Response(text=response_text.strip())
            
        except Exception as e:
            print(f"‚ùå ExLlamaV2 generation error: {e}")
            return ExLlamaV2Response(text="Error in generation")
        
        finally:
            # Clear GPU memory
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def cleanup(self):
        """Clean up resources"""
        if hasattr(self, 'model_instance'):
            del self.model_instance
        if hasattr(self, 'cache'):
            del self.cache
        if hasattr(self, 'generator'):
            del self.generator
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()



















