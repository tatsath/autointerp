================================================================================
Feature 1532 - Explanation vs Activating Words Analysis
================================================================================

Explanation: "Special dividend declarations by funds"

================================================================================
TOP ACTIVATING WORDS (from token analysis)
================================================================================
These are the individual tokens/words that fire most strongly:

 1. 'Today'                              max_act=  32.25  count=    4
 2. 'Senior'                             max_act=  32.25  count=    2
 3. 'addition'                           max_act=  32.25  count=    1
 4. 'Third'                              max_act=  32.00  count=   27
 5. 'Should'                             max_act=  32.00  count=   24
 6. 'Sector'                             max_act=  32.00  count=   22
 7. 'Industry'                           max_act=  32.00  count=   14
 8. 'Industries'                         max_act=  32.00  count=    9
 9. 'Industrial'                         max_act=  32.00  count=    5
10. 'Good'                               max_act=  32.00  count=    5
11. 'Nor'                                max_act=  32.00  count=    2
12. 'Rico'                               max_act=  32.00  count=    2
13. 'Instead'                            max_act=  32.00  count=    1
14. 'Rid'                                max_act=  32.00  count=    1
15. 'Isn'                                max_act=  32.00  count=    1
16. 'Needs'                              max_act=  32.00  count=    1
17. 'Cons'                               max_act=  32.00  count=    1
18. 'Minor'                              max_act=  32.00  count=    1
19. 'Pros'                               max_act=  32.00  count=    1
20. 'ider'                               max_act=  31.75  count=  171
21. 'Interest'                           max_act=  31.75  count=   62
22. 'Season'                             max_act=  31.75  count=   56
23. 'Contin'                             max_act=  31.75  count=   25
24. 'Year'                               max_act=  31.75  count=   17
25. 'Again'                              max_act=  31.75  count=    7
26. 'Well'                               max_act=  31.75  count=    6
27. 'Prospect'                           max_act=  31.75  count=    5
28. 'Consider'                           max_act=  31.75  count=    4
29. 'Great'                              max_act=  31.75  count=    4
30. 'Issues'                             max_act=  31.75  count=    3


================================================================================
EXAMPLES THE EXPLANATION MODEL SAW
================================================================================
These are the full text examples (with context) that the LLM explainer analyzed:


Example 1 (max_activation=9.625):
ActivatingExample(tokens=tensor([  1462,  12243,   1879,  52497,     12,   5119,   1077,   9335,  26507,
         90818,  17703,  85112,   1659,   1048,   1046,   1048,   1049,   6061,
        110559,     12,  10338,   1105,   1798,   1317,   3691,   1372,  17267,
          6170,   1317,   1359,  13125,   3571]), activations=tensor([4.1562, 3.1719, 4.7188, 1.0156, 2.6719, 3.9062, 1.0781, 4.7500, 4.6562,
        7.8125, 7.8125, 3.8125, 8.3750, 3.8438, 9.6250, 4.6250, 4.0000, 3.0625,
        7.1250, 3.5625, 2.9844, 2.4062, 1.3438, 4.7188, 4.4375, 2.1562, 2.7656,
        2.2031, 2.9062, 4.5312, 2.0781, 5.7812], dtype=torch.float16), normalized_activations=tensor([2., 1., 2., 1., 1., 2., 1., 2., 2., 3., 3., 2., 3., 2., 3., 2., 2., 1.,
        3., 2., 1., 1., 1., 2., 2., 1., 1., 1., 1., 2., 1., 2.],
       dtype=torch.float16), str_tokens=[' -', ' Anal', 'yst', ' Blog', '<SPECIAL_12>', 'PI', 'M', 'CO', ' Municipal', ' Income', ' Fund', ' declares', ' $', '0', '.', '0', '1', ' special', ' dividend', '<SPECIAL_12>', 'Card', 'i', 'ome', ' to', ' div', 'est', ' Canadian', ' business', ' to', ' C', 'ipher', ' Ph'], quantile=0)
--------------------------------------------------------------------------------

Example 2 (max_activation=29.75):
ActivatingExample(tokens=tensor([ 52810,  27330,   3249,   1531,  67867,   1656, 101578,   1032,   1053,
          1048,   3626,  85866,   1681,   1407,   3813,     12,   6720,  90598,
        122959,   1039,  66971,  21404,   6672,   4454,  27944,   1421,  32868,
         12569,     12,   1069, 116011,  21536]), activations=tensor([29.7500,  1.1719,  2.5625,  3.8750,  2.3906,  2.1406,  2.5469,  1.0625,
         1.8594,  2.7344,  2.2344,  2.2500,  2.3125,  2.9531,  0.7227,  2.5625,
         1.8281,  3.4375,  3.0781,  3.9062,  3.2969,  1.9844,  1.6719,  0.8242,
         2.3281, -2.0156,  2.1250,  1.1719,  1.4219,  3.0625,  0.7852,  3.5000],
       dtype=torch.float16), normalized_activations=tensor([10.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
         1.,  1.,  1.,  2.,  1.,  2.,  2.,  1.,  1.,  1.,  1., -0.,  1.,  1.,
         1.,  1.,  1.,  2.], dtype=torch.float16), str_tokens=['Break', 'outs', ' On', ' The', ' Rise', ' In', ' IBD', ' ', '5', '0', ' But', ' Francesca', "'s", ' D', 'ives', '<SPECIAL_12>', 'My', 'riad', ' Genetics', "'", ' Positive', ' Study', ' Data', ' May', ' Exp', 'and', ' Customer', ' Base', '<SPECIAL_12>', 'E', '&P', ' Imp'], quantile=0)
--------------------------------------------------------------------------------

Example 3 (max_activation=28.375):
ActivatingExample(tokens=tensor([ 27540,   1681,  33637,  24102,   1302,     12,  34279,  81669,   5916,
          1436,  47466,  10306,   1044,   3626,  18196,   8250,   2813,  77626,
         67867,     12,  10884,   2111,   3870,   6470,  83784,  25839,  29960,
         35316, 113443,   3079,  16035,     12]), activations=tensor([28.3750, -1.1172,  4.7500,  2.3281, -0.1094,  1.6406, -1.0938, -0.1367,
         0.0664,  0.2266,  4.9062,  0.4336,  1.2656,  0.9961,  1.2188,  2.6406,
         3.0312,  3.1250,  2.7188,  0.3320,  1.9375,  0.7383,  3.6562,  0.5820,
        -1.4062,  0.4727,  1.2344, -0.0820,  1.0625,  2.7656,  1.4375, -0.1055],
       dtype=torch.float16), normalized_activations=tensor([9., -0., 2., 1., -0., 1., -0., -0., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 2., 1., -0., 1., 1., -0., 1., 1., 1., -0.],
       dtype=torch.float16), str_tokens=['zek', "'s", ' Daily', ' Brief', 'ing', '<SPECIAL_12>', 'Win', 'neb', 'ago', ' E', 'arnings', ' Miss', ',', ' But', ' Stock', ' Up', ' As', ' Orders', ' Rise', '<SPECIAL_12>', 'Icon', 'ix', ' To', ' Get', ' Fashion', ' Brand', ' Lic', 'ensing', ' Boost', ' Ab', 'road', '<SPECIAL_12>'], quantile=0)
--------------------------------------------------------------------------------

Example 4 (max_activation=30.875):
ActivatingExample(tokens=tensor([ 18545,   1982,   1058,   7633,   3794,   1871,  19549,   1115,   1659,
          1053,   1048,   1059,  29701,   1044,   1335,   4030,   1270,   1436,
         47466,   8867,  25418,   1462,  12243,   1879,  52497,     12,   5528,
         18392,   1392,   1716, 118443,   1319]), activations=tensor([30.8750, -1.5234, -0.4238,  1.2500,  0.9492,  2.4688, -0.0781, -2.6406,
         2.3594,  1.3125,  1.6562,  0.8164,  1.5938,  5.6875,  5.4688,  4.3438,
         5.8438,  3.5000,  6.3125,  4.5312,  0.2891,  2.7656,  3.5000,  4.5938,
         4.0312,  3.3750,  1.7969,  3.7188,  4.7500,  4.8125,  5.5938, -1.2969],
       dtype=torch.float16), normalized_activations=tensor([10., -0., -0.,  1.,  1.,  1., -0., -0.,  1.,  1.,  1.,  1.,  1.,  2.,
         2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  2.,  2.,  2.,  2.,  1.,  2.,
         2.,  2.,  2., -0.], dtype=torch.float16), str_tokens=[' Round', 'up', ':', ' Cr', 'ude', ' Re', 'claim', 's', ' $', '5', '0', ';', ' BP', ',', ' S', 'unc', 'or', ' E', 'arnings', ' Dis', 'appoint', ' -', ' Anal', 'yst', ' Blog', '<SPECIAL_12>', 'Up', '/D', 'ow', 'ng', 'rades', ' ('], quantile=0)
--------------------------------------------------------------------------------

Example 5 (max_activation=8.875):
ActivatingExample(tokens=tensor([  2760,  24729,   1032,   1051,   1046,   1051,   1037,   2453,   9730,
         23447,  16896,     12,   1052, 121259,  18196,   1115,   1317, 113443,
         41661,   2858,   5930,   1261,  33446,   1944,   1436,  47466,  24814,
            12,  59119,  55397,   1685,  78439]), activations=tensor([ 5.5938,  2.3750,  0.9023,  1.2812,  0.8633, -0.0312,  4.1562,  7.2188,
         5.0000,  6.0312,  4.0312,  2.5312, -0.3828, -2.3281,  4.9375,  2.5469,
         7.4375,  4.3438,  2.1719,  0.2148,  4.4062,  3.6875,  4.5625,  1.3750,
         6.0625,  8.8750,  3.8750,  3.3438,  3.3750,  1.5625,  1.7188,  3.9062],
       dtype=torch.float16), normalized_activations=tensor([2., 1., 1., 1., 1., -0., 2., 3., 2., 2., 2., 1., -0., -0., 2., 1., 3., 2.,
        1., 1., 2., 2., 2., 1., 2., 3., 2., 2., 2., 1., 1., 2.],
       dtype=torch.float16), str_tokens=['ty', ' falls', ' ', '3', '.', '3', '%', ' after', ' share', ' offering', ' starts', '<SPECIAL_12>', '4', ' Retail', ' Stock', 's', ' to', ' Boost', ' Sent', 'iment', ' With', ' a', ' Lik', 'ely', ' E', 'arnings', ' Beat', '<SPECIAL_12>', 'Consumer', ' Cycl', 'ical', ' Sector'], quantile=1)
--------------------------------------------------------------------------------

Example 6 (max_activation=9.0):
ActivatingExample(tokens=tensor([  1462,   2466,   1052,   1032,   1050,   1048,   1049,   1053,  16104,
            12,  44345,  24450, 120447,   1039,   1319,   1077,  12001,   1041,
          2466,   1050,   1436,  47466,  24814,  75981,   2130,   1044,   8962,
          8250,     12,   7493, 107626,   6672]), activations=tensor([ 1.5000e+00,  4.9375e+00,  3.1562e+00,  2.9688e+00,  8.2500e+00,
         3.2422e-01,  1.9688e+00,  4.1875e+00, -1.1719e-01,  1.9688e+00,
         2.7500e+00,  3.7812e+00,  8.9844e-02,  3.0156e+00,  3.8438e+00,
         3.7188e+00,  1.9062e+00,  3.5312e+00,  4.7812e+00,  2.9219e+00,
         4.2812e+00,  9.0000e+00, -4.5703e-01,  1.7500e+00,  4.7812e+00,
         6.4062e+00,  2.7031e+00,  3.5938e+00,  3.0000e+00,  1.1250e+00,
        -1.1719e-01,  7.8125e-03], dtype=torch.float16), normalized_activations=tensor([1., 2., 1., 1., 3., 1., 1., 2., -0., 1., 1., 2., 1., 1., 2., 2., 1., 2.,
        2., 1., 2., 3., -0., 1., 2., 2., 1., 2., 1., 1., -0., 1.],
       dtype=torch.float16), str_tokens=[' -', ' Q', '4', ' ', '2', '0', '1', '5', ' Update', '<SPECIAL_12>', 'Mer', 'itage', ' Homes', "'", ' (', 'M', 'TH', ')', ' Q', '2', ' E', 'arnings', ' Beat', ' Estim', 'ates', ',', ' View', ' Up', '<SPECIAL_12>', 'What', ' Makes', ' Data'], quantile=1)
--------------------------------------------------------------------------------

Example 7 (max_activation=9.3125):
ActivatingExample(tokens=tensor([  1576,  60539, 111132,   1039,  39126,   3581,     12,   6882,   2938,
          5930,  19607,  84452,   4172,   1044,  32290,   9214, 118747, 117605,
          2717,     12,   5528,   9500,   1436,  47466,  67533,   1317,  34842,
          1058,  73022,   1044,  17757,   1044]), activations=tensor([2.9219, 5.1875, 5.2188, 4.0000, 5.6875, 3.7188, 4.2812, 3.5000, 2.4062,
        4.5938, 4.7500, 3.5625, 4.2812, 5.0625, 5.5000, 2.7656, 3.5625, 5.6250,
        4.0625, 2.2656, 2.5938, 3.4688, 6.9688, 9.3125, 8.6250, 7.5000, 1.2812,
        1.7656, 3.9688, 2.3281, 5.4062, 6.1562], dtype=torch.float16), normalized_activations=tensor([1., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 1., 2., 2.,
        2., 1., 1., 2., 3., 3., 3., 3., 1., 1., 2., 1., 2., 2.],
       dtype=torch.float16), str_tokens=[" '", 'Flash', ' Crash', "'", ' Flash', 'back', '<SPECIAL_12>', 'Inter', 'view', ' With', ' Brian', ' Cul', 'ley', ',', ' CEO', ' Of', ' Mast', ' Therapeut', 'ics', '<SPECIAL_12>', 'Up', 'coming', ' E', 'arnings', ' Reports', ' to', ' Watch', ':', ' JD', ',', ' HD', ','], quantile=1)
--------------------------------------------------------------------------------

Example 8 (max_activation=8.8125):
ActivatingExample(tokens=tensor([  1067,  10261, 114831,     12,   1784,  30386,   5849,   1466, 126488,
          1058,   1349,  96111,   9214,  40035,     12,  42131,  67907,   1937,
         29897,  77859,   1317,   1871,   1664,   1659,   1882,   2096,   1674,
          1055,   1057,  60115,  64860,   1536]), activations=tensor([3.7812, 3.4375, 5.0625, 2.3438, 2.5625, 2.1406, 2.9062, 4.3438, 2.7812,
        0.2656, 2.3906, 4.4062, 4.3750, 5.2500, 4.1250, 1.0938, 3.4375, 1.3125,
        1.9062, 0.2305, 1.6719, 4.2188, 1.4375, 4.4062, 0.1953, 0.6055, 2.9375,
        1.1406, 2.1094, 2.2188, 8.8125, 2.0625], dtype=torch.float16), normalized_activations=tensor([2., 2., 2., 1., 1., 1., 1., 2., 1., 1., 1., 2., 2., 2., 2., 1., 2., 1.,
        1., 1., 1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 3., 1.],
       dtype=torch.float16), str_tokens=['C', 'rest', ' Mines', '<SPECIAL_12>', 'The', ' Micro', '-C', 'ap', ' Digest', ':', ' A', ' Margin', ' Of', ' Safety', '<SPECIAL_12>', 'Air', ' Fresh', 'ener', ' Market', ' Expected', ' to', ' Re', 'ach', ' $', '–', "',", '—', '7', '9', '.–', ' Million', ' by'], quantile=1)
--------------------------------------------------------------------------------

Example 9 (max_activation=8.5625):
ActivatingExample(tokens=tensor([ 17488,  31762,     12, 117414,   2029,  59825,  41480, 106315,   1908,
          1294,   3140,  11841,  76632,   1462,  12243,   1879,  52497,     12,
         69031,  44172,   1343,  34127,  30126,   1294,   6032,     12,   1053,
         44537,   2994,   4728,   1394,  44692]), activations=tensor([ 2.8281,  7.8750,  3.5000,  1.8906,  3.3750,  4.9688,  5.2812,  8.5625,
         6.1562,  5.4688,  6.5938,  5.2500,  6.4688,  4.9375,  5.0312,  4.8750,
         6.0000,  2.9688,  1.0156,  2.6406,  0.6445, -0.8750,  0.6836,  2.8281,
         4.3438,  2.6406,  0.5820,  6.0000,  4.9375,  4.2188,  3.0156,  5.5000],
       dtype=torch.float16), normalized_activations=tensor([1., 3., 2., 1., 2., 2., 2., 3., 2., 2., 3., 2., 3., 2., 2., 2., 2., 1.,
        1., 1., 1., -0., 1., 1., 2., 1., 1., 2., 2., 2., 1., 2.],
       dtype=torch.float16), str_tokens=[' Ten', ' Years', '<SPECIAL_12>', 'Vals', 'par', ' Partners', ' Louisiana', '-Pac', 'ific', ' in', ' New', ' Color', ' Launch', ' -', ' Anal', 'yst', ' Blog', '<SPECIAL_12>', 'Chinese', ' inflation', ' g', 'athers', ' pace', ' in', ' November', '<SPECIAL_12>', '5', ' Technical', ' Tr', 'ades', ' for', ' Tuesday'], quantile=2)
--------------------------------------------------------------------------------

Example 10 (max_activation=8.1875):
ActivatingExample(tokens=tensor([  1057,   1047,   1049,   1053,   1041,     12,   1070,  79253,  30768,
         54372,   1063,   1319,  34907,   1115,   3870,  34842,  17510,   8799,
          1041,     12,  18741, 109765,   1088,  10889,   1319,   1084,   1074,
          1088,   1041,  24814,   1436,  47466]), activations=tensor([ 5.0938,  3.2812, -3.5625,  4.0625,  2.4375,  2.0156,  2.9062,  1.7031,
         6.7812,  4.3750,  3.8125,  1.3281,  5.3125,  3.9375,  8.1875,  5.0625,
         5.3125,  4.6250,  2.4688,  3.0000, -0.9648,  3.7188,  0.9883,  2.2812,
         3.0781,  6.0000,  3.2969,  3.2656,  3.2500,  1.4531,  4.1562,  7.6875],
       dtype=torch.float16), normalized_activations=tensor([2., 2., 0., 2., 1., 1., 1., 1., 3., 2., 2., 1., 2., 2., 3., 2., 2., 2.,
        1., 1., -0., 2., 1., 1., 1., 2., 2., 2., 2., 1., 2., 3.],
       dtype=torch.float16), str_tokens=['9', '/', '1', '5', ')', '<SPECIAL_12>', 'F', 'avorable', ' Trade', ' Winds', '?', ' (', 'Stock', 's', ' To', ' Watch', ' Pod', 'cast', ')', '<SPECIAL_12>', 'Will', ' TJ', 'X', ' Company', ' (', 'T', 'J', 'X', ')', ' Beat', ' E', 'arnings'], quantile=2)
--------------------------------------------------------------------------------

Example 11 (max_activation=8.25):
ActivatingExample(tokens=tensor([120768,   1443,  35834,   7417,   1327,   1474,  23008,  19660,   1462,
         12243,   1879,  52497,     12,  10107,  94820,   1317,  27944,   1421,
         44760,  14898,   1294,  13734,  50219,   6627,   3083,     12,   3074,
          1065,   3905,  94649,   1319,   3074]), activations=tensor([ 2.9688,  5.6562,  3.1406,  6.2500,  8.2500,  6.7812,  5.5312,  8.0625,
         6.2500,  2.0156,  3.2344,  1.0156,  2.6562,  4.2188,  4.6562,  5.1875,
         5.2500,  2.5938,  4.5312,  5.8438,  4.9688,  2.3125, -0.2832,  0.6211,
         1.2500,  2.6719,  2.7031,  1.7500,  1.7031,  2.4844,  2.9219,  3.3750],
       dtype=torch.float16), normalized_activations=tensor([1., 2., 1., 2., 3., 3., 2., 3., 2., 1., 2., 1., 1., 2., 2., 2., 2., 1.,
        2., 2., 2., 1., -0., 1., 1., 1., 1., 1., 1., 1., 1., 2.],
       dtype=torch.float16), str_tokens=['andria', ' H', 'ikes', ' Div', 'id', 'end', ' Yet', ' Again', ' -', ' Anal', 'yst', ' Blog', '<SPECIAL_12>', 'First', ' Horizon', ' to', ' Exp', 'and', ' Branch', ' Network', ' in', ' Key', ' Growth', ' Mark', 'ets', '<SPECIAL_12>', 'ST', 'A', 'AR', ' Surgical', ' (', 'ST'], quantile=2)
--------------------------------------------------------------------------------

Example 12 (max_activation=8.125):
ActivatingExample(tokens=tensor([ 33209,   6627,   3083,   5417,   1531,   1608,   3051,   1046,  81160,
         58410,   1058,   4925,   1531,  80588,  58268,   2013,     12,   4944,
          1786,  30606,   4146,   2813,   6658,   1045,   2892,   9878,   1656,
          6672,  83826,     12,  60221, 108924]), activations=tensor([8.1250, 2.5938, 0.3711, 7.6250, 5.8125, 2.5156, 6.3438, 5.5938, 6.5938,
        0.2383, 0.8477, 1.3594, 1.5625, 1.7344, 2.8594, 1.7969, 4.3438, 2.3281,
        0.3398, 4.5312, 1.9531, 3.0312, 4.8438, 4.3438, 1.6250, 4.9688, 4.0625,
        2.2188, 2.3438, 2.1250, 3.6875, 5.0000], dtype=torch.float16), normalized_activations=tensor([3., 1., 1., 3., 2., 1., 2., 2., 3., 1., 1., 1., 1., 1., 1., 1., 2., 1.,
        1., 2., 1., 1., 2., 2., 1., 2., 2., 1., 1., 1., 2., 2.],
       dtype=torch.float16), str_tokens=[' Bond', ' Mark', 'ets', ' After', ' The', ' U', '.S', '.', ' Presidential', ' Election', ':', ' When', ' The', ' Dust', ' Sett', 'les', '<SPECIAL_12>', 'Table', 'au', ' Emer', 'ges', ' As', ' Go', '-', 'To', ' Name', ' In', ' Data', ' Analytics', '<SPECIAL_12>', 'Lee', ' Enterprises'], quantile=2)
--------------------------------------------------------------------------------

Example 13 (max_activation=7.9375):
ActivatingExample(tokens=tensor([ 98084,   8284,   1115,   1510,     12,   3761,  49492,  29897,  75981,
          2130,   1871,  99297, 127163,   1411,  24919,  17909,   2048,   3797,
         98084,  22143,     12,   1051,   1037,   3316,   1531,   3140,   1032,
          1053,   1037,   1058,   1032,   1051]), activations=tensor([ 5.2188,  4.7188,  3.2500,  3.7188,  2.1562,  3.3438,  2.1094,  6.2188,
         2.7344,  1.1250,  3.7188,  0.7930,  1.6406,  0.0625,  1.8125,  2.1719,
         1.9062,  2.7812,  2.1406,  3.9688,  3.1406,  0.1172,  4.0938,  2.6562,
        -0.2715,  1.6094,  4.0312,  4.5000,  7.9375,  5.4062,  3.3750,  1.8594],
       dtype=torch.float16), normalized_activations=tensor([2., 2., 2., 2., 1., 2., 1., 2., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,
        1., 2., 1., 1., 2., 1., -0., 1., 2., 2., 3., 2., 2., 1.],
       dtype=torch.float16), str_tokens=['-Time', ' High', 's', '…', '<SPECIAL_12>', 'De', 'veloped', ' Market', ' Estim', 'ates', ' Re', 'visions', ' Bread', 'th', ' Has', ' Hit', ' An', ' All', '-Time', ' Low', '<SPECIAL_12>', '3', '%', ' Is', ' The', ' New', ' ', '5', '%', ':', ' ', '3'], quantile=3)
--------------------------------------------------------------------------------

Example 14 (max_activation=7.75):
ActivatingExample(tokens=tensor([127469,   1059,   9554,   5916,   8868,   5607,   6438,  12145,  47466,
            12,   1087,   9327,   1439,   8853,  39985,  77546,  69999,  52932,
            12,   4271,  12082,   1461,   8957,  23361, 107306,   9214,  32000,
         12145,   7373,  13336,  27988,     12]), activations=tensor([ 1.7500,  1.8750,  4.2188,  0.3164,  4.3438, -0.5742,  4.0625,  4.0312,
         7.7500,  1.8594,  2.2500,  3.5000,  3.7500,  4.2188,  3.3750,  1.1875,
         4.9688,  0.4336,  2.0000,  2.7500,  0.7539,  5.0938, -0.2344, -0.2754,
         5.9375,  1.0469,  7.4375,  3.3438, -1.1562,  6.1562,  1.4688,  0.8555],
       dtype=torch.float16), normalized_activations=tensor([1., 1., 2., 1., 2., -0., 2., 2., 3., 1., 1., 2., 2., 2., 2., 1., 2., 1.,
        1., 1., 1., 2., -0., -0., 2., 1., 3., 2., -0., 2., 1., 1.],
       dtype=torch.float16), str_tokens=[' Losses', ';', ' Av', 'ago', ' Sl', 'ips', ' Pre', '-E', 'arnings', '<SPECIAL_12>', 'W', 'ells', ' F', 'argo', ' cuts', ' Sprint', ' subscriber', ' forecast', '<SPECIAL_12>', 'Service', 'Now', ' G', 'rows', ' Through', ' Addition', ' Of', ' Capital', '-E', 'fficient', ' Start', 'ups', '<SPECIAL_12>'], quantile=3)
--------------------------------------------------------------------------------

Example 15 (max_activation=7.6875):
ActivatingExample(tokens=tensor([ 1050,  1048,  1049,  1056,  1058,  2409, 33969, 68017, 17703, 24919,
        83551,  1464,  3022,  3060, 11560, 31652,  1541,    12,  1080,  1297,
         2511,  1867,  1051,  1037,  3009,  2466,  1050,  3657, 20181,  1059,
         8222,  2466]), activations=tensor([1.7500, 3.0156, 2.3750, 3.5312, 0.6680, 2.3594, 4.6562, 7.6875, 2.0625,
        0.0195, 4.0625, 3.8750, 4.4062, 5.2188, 2.7969, 7.3750, 6.7188, 4.0625,
        2.8281, 2.2344, 1.1250, 3.2188, 0.4648, 2.1562, 5.4062, 4.5938, 4.2500,
        6.6562, 1.5000, 3.2188, 5.9688, 5.3125], dtype=torch.float16), normalized_activations=tensor([1., 1., 1., 2., 1., 1., 2., 3., 1., 1., 2., 2., 2., 2., 1., 3., 3., 2.,
        1., 1., 1., 1., 1., 1., 2., 2., 2., 3., 1., 1., 2., 2.],
       dtype=torch.float16), str_tokens=['2', '0', '1', '8', ':', ' This', ' Senior', ' Loan', ' Fund', ' Has', ' Increasing', ' N', 'II', ' And', ' Great', ' Cover', 'age', '<SPECIAL_12>', 'P', 'ent', 'air', ' +', '3', '%', ' post', ' Q', '2', ' results', ' beat', ';', ' provides', ' Q'], quantile=3)
--------------------------------------------------------------------------------

Example 16 (max_activation=8.0625):
ActivatingExample(tokens=tensor([ 2251,  1510,  1882,  1056, 28678, 99559,  1795,  1394, 55716,  1044,
        58609, 40946, 59825,  1044,  6421,    12,  1078, 64841,  1319,  1078,
        13257,  1041,  2466,  1051,  1436, 47466,  1058,  4971, 70979, 19658,
         1349,  7646]), activations=tensor([-0.9844, -0.7305, -1.0469, -0.9961, -0.4102,  2.3281,  4.2500,  2.0312,
         4.8125,  5.5625,  4.9688,  5.8438,  5.6875,  7.0938,  2.1719,  1.8438,
         2.7031, -0.1133,  2.6406,  4.5938,  2.0781,  3.0938,  4.3125,  2.5312,
         4.2188,  8.0625,  4.3750,  2.2812, -0.3594, -0.7422,  3.0156, -0.2090],
       dtype=torch.float16), normalized_activations=tensor([-0., -0., -0., -0., -0., 1., 2., 1., 2., 2., 2., 2., 2., 3., 1., 1., 1., -0.,
        1., 2., 1., 1., 2., 1., 2., 3., 2., 1., -0., -0., 1., -0.],
       dtype=torch.float16), str_tokens=[' —', '…', '–', '8', ' Summary', ' Expect', 'ations', ' for', ' Discovery', ',', ' Focus', ' Financial', ' Partners', ',', ' ...', '<SPECIAL_12>', 'N', 'okia', ' (', 'N', 'OK', ')', ' Q', '3', ' E', 'arnings', ':', ' Will', ' Networks', ' Unit', ' A', 'ffect'], quantile=3)
--------------------------------------------------------------------------------

Example 17 (max_activation=7.5625):
ActivatingExample(tokens=tensor([16104,  1032,  1051,  1049,  1058,  1032,  1051, 11630,  1292,  6112,
        24007,  2383,  5646,  4425, 19008,  3213,  7164,    12,  1784, 10327,
        16048,  1773,  1279, 10021,  3060, 10938,  4957,  1286,  7417,  1327,
         1474, 25667]), activations=tensor([ 4.7188,  1.0312,  3.1875,  1.5781,  1.6562,  0.7383,  2.5625,  5.4688,
         3.9375,  6.0625,  3.7812,  5.0938,  4.2500,  3.7812,  0.5820,  0.6289,
         1.2500,  3.8438,  2.9062,  5.0625,  3.5938,  1.3594,  1.1719, -1.7344,
         7.5625,  7.0312,  2.3281, -1.6641,  2.8438,  5.8438,  3.5312,  3.3125],
       dtype=torch.float16), normalized_activations=tensor([2., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 1., 1., 1., 2.,
        1., 2., 2., 1., 1., -0., 3., 3., 1., -0., 1., 2., 2., 2.],
       dtype=torch.float16), str_tokens=[' Update', ' ', '3', '1', ':', ' ', '3', ' Bill', 'ion', 'aire', ' Hab', 'its', ' That', ' Can', ' Make', ' You', ' Rich', '<SPECIAL_12>', 'The', ' Most', ' Und', 'erv', 'al', 'ued', ' And', ' Over', 'valu', 'ed', ' Div', 'id', 'end', ' Champions'], quantile=4)
--------------------------------------------------------------------------------

Example 18 (max_activation=7.5625):
ActivatingExample(tokens=tensor([  2466,   1073,   1072,   1085,   1044,   1434,  13339,   1044,  42981,
          9335,   1044, 109364,   1088,   1044,  77811,   1086,     12,  25238,
          3794,  10673,  75713,   1809,  15202,   1454,   6732,  32260,   6410,
            12,   1086, 116448,   1317,  14688]), activations=tensor([ 3.9375,  3.4688,  3.9062,  5.4375,  7.5625,  4.4688,  1.5938,  5.3125,
         4.4062,  4.4062,  6.1562,  3.0938,  2.4688,  5.5625,  4.4375,  4.7188,
         3.4375,  3.1094,  1.7969,  1.0625, -0.5664,  4.5312,  2.5625,  0.6758,
        -1.6250,  6.1250,  2.1719,  2.7969,  3.0938,  1.6875,  6.3125,  6.2812],
       dtype=torch.float16), normalized_activations=tensor([2., 2., 2., 2., 3., 2., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 1.,
        1., 1., -0., 2., 1., 1., -0., 2., 1., 1., 1., 1., 2., 2.],
       dtype=torch.float16), str_tokens=[' Q', 'I', 'H', 'U', ',', ' R', 'AX', ',', ' VE', 'CO', ',', ' MDR', 'X', ',', ' IMP', 'V', '<SPECIAL_12>', 'Cr', 'ude', ' oil', ' rebounds', ' but', ' ends', ' with', ' big', ' weekly', ' loss', '<SPECIAL_12>', 'V', 'entas', ' to', ' buy'], quantile=4)
--------------------------------------------------------------------------------

Example 19 (max_activation=7.625):
ActivatingExample(tokens=tensor([ 73989,   2981, 116095,   2898,   5187,   1032,   1049,   1048,     12,
         13148,   1105,  15216,   1044,  11884,   1046,   1319,   1086,  19693,
          1078,   1041,  32290,  12119,  84159,  34840,   1408,   2466,   1052,
          1032,   1050,   1048,   1049,   1056]), activations=tensor([ 3.3125,  7.6250, -0.6797,  1.0625,  2.4062,  5.0312,  0.5430,  3.7188,
         4.8750,  3.4688,  1.5938, -0.6680,  5.4062,  4.1562,  2.0469,  3.1875,
         3.6875,  1.5000,  1.4844,  3.1562,  3.0000,  3.5312,  2.9844,  1.8281,
         3.0156,  3.7812,  2.3438,  2.8750,  0.2305, -0.6328, -1.8906,  2.8438],
       dtype=torch.float16), normalized_activations=tensor([2., 3., -0., 1., 1., 2., 1., 2., 2., 2., 1., -0., 2., 2., 1., 1., 2., 1.,
        1., 1., 1., 2., 1., 1., 1., 2., 1., 1., 1., -0., -0., 1.],
       dtype=torch.float16), str_tokens=['Techn', 'ically', ' Speaking', ' For', ' August', ' ', '1', '0', '<SPECIAL_12>', 'Ver', 'i', 'Sign', ',', ' Inc', '.', ' (', 'V', 'RS', 'N', ')', ' CEO', ' Jim', ' Bid', 'zos', ' on', ' Q', '4', ' ', '2', '0', '1', '8'], quantile=4)
--------------------------------------------------------------------------------

Example 20 (max_activation=7.5625):
ActivatingExample(tokens=tensor([32450,    12,  2596, 70553,  1058, 34962,  1911, 67858, 55123,  1848,
        72892, 11299,  1044,  7744, 15765,    12,  7848, 38052, 48912,  1099,
         5258,  1890,  1581, 94496, 46878,  3932,  2491, 49492,  6627,  3083,
         1044,  1623]), activations=tensor([7.5000, 5.4688, 2.7500, 2.8281, 1.2031, 3.7812, 1.6094, 2.9688, 3.3750,
        2.7656, 4.1250, 7.5625, 6.5312, 6.3750, 7.0312, 3.3438, 2.6250, 4.4062,
        3.5938, 1.5000, 3.0938, 2.2812, 2.6562, 3.1094, 3.5625, 0.9180, 4.6250,
        0.1758, 3.9688, 2.3750, 4.5625, 2.4062], dtype=torch.float16), normalized_activations=tensor([3., 2., 1., 1., 1., 2., 1., 1., 2., 1., 2., 3., 3., 2., 3., 2., 1., 2.,
        2., 1., 1., 1., 1., 1., 2., 1., 2., 1., 2., 1., 2., 1.],
       dtype=torch.float16), str_tokens=[' Tech', '<SPECIAL_12>', 'Re', 'uters', ':', ' Perm', 'ian', ' Basin', ' boom', ' out', 'paces', ' supply', ',', ' transport', ' networks', '<SPECIAL_12>', 'True', 'wealth', ' Ll', 'c', ' Bu', 'ys', ' V', 'anguard', ' FT', 'SE', ' De', 'veloped', ' Mark', 'ets', ',', ' i'], quantile=4)
--------------------------------------------------------------------------------


================================================================================
ANALYSIS: Why the Discrepancy?
================================================================================

Key Differences:

1. EXPLANATION MODEL SEES:
   - Full text sequences (32 tokens of context)
   - Examples centered on activation points
   - Semantic context and relationships between words
   - Top 15-25 strongest activating examples

2. TOKEN ANALYSIS SHOWS:
   - Individual tokens/words in isolation
   - Top 50k strongest activations sampled
   - No context about surrounding words
   - Frequency-based ranking

3. WHY THEY DIFFER:
   - The explanation model looks at FULL CONTEXT and SEMANTIC MEANING
   - It sees patterns like 'Special dividend declarations by funds' in context
   - Individual words like 'Industry', 'Third', 'Should' may appear frequently
     but the explanation captures the SEMANTIC PATTERN across examples
   - The LLM explainer is doing semantic abstraction, not just word counting

4. EXAMPLE INTERPRETATION:
   Looking at the examples above, the explanation model likely saw:
   - Patterns related to financial declarations, announcements
   - Context about funds, dividends, special payments
   - Even if individual words are common, the COMBINATION and CONTEXT
     suggests the semantic pattern of 'special dividend declarations'

5. CONCLUSION:
   The explanation is a SEMANTIC INTERPRETATION of what the feature captures,
   while the top words are STATISTICAL FREQUENCIES. They complement each other:
   - Top words show WHAT tokens fire most
   - Explanation shows WHAT SEMANTIC PATTERN those tokens represent in context
